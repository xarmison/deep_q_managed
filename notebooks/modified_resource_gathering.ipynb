{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Resource Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import mo_gymnasium as mo_gym\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "        A replay buffer class for storing and sampling transitions for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int, input_shape: list) -> None:\n",
    "        \"\"\"\n",
    "            Initializes the ReplayBuffer class.\n",
    "\n",
    "            Parameters:\n",
    "                - max_size (int): The maximum size of the replay buffer.\n",
    "                - input_shape (list): The shape of the input state.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\" \n",
    "        \n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "        self.state_memory = np.zeros(\n",
    "            (self.mem_size, *input_shape),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.new_state_memory = np.zeros(\n",
    "            (self.mem_size, *input_shape),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "        # Mask to discount potential features rewards that may come after the current state\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state: np.array, action: int, reward: float, state_: np.array, done: bool) -> None:\n",
    "        \"\"\"\n",
    "            Stores a transition in the replay memory.\n",
    "\n",
    "            Parameters:\n",
    "                - state (np.array): The current state of the environment.\n",
    "                - action (int): The action taken in the current state.\n",
    "                - reward (float): The reward received for taking the action.\n",
    "                - state_ (np.array): The next state of the environment.\n",
    "                - done (bool): Indicates whether the episode is done after taking the action.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        # Index of first free memory\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        # Stores the transition on the memories in the indices in the appropriate arrays\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size: int) -> tuple:\n",
    "        \"\"\"\n",
    "            Randomly samples a batch of transitions from the replay memory buffer.\n",
    "\n",
    "            Args:\n",
    "                batch_size (int): The number of transitions to sample.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple containing the sampled states, actions, rewards, next states, and terminal flags.\n",
    "        \"\"\"\n",
    "\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDeepNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        A class for a dueling deep neural network for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float, n_actions: int, input_dims: list, name: str, chkpt_dir: str) -> None:\n",
    "        \"\"\"\n",
    "            Initializes the DuelingDeepNetwork class.\n",
    "\n",
    "            Parameters:\n",
    "                - learning_rate (float): The learning rate for the optimizer.\n",
    "                - n_actions (int): The number of actions in the environment.\n",
    "                - input_dims (list): The dimensions of the input state.\n",
    "                - name (str): The name of the network.\n",
    "                - chkpt_dir (str): The directory to save the network's checkpoints.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        super(DuelingDeepNetwork, self).__init__()\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.chkpt_file = os.path.join(self.chkpt_dir, self.name)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(*input_dims, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Linear(512, 1)\n",
    "        self.advantage = nn.Linear(512, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state: T.Tensor) -> tuple[T.Tensor, T.Tensor]:\n",
    "        \"\"\"\n",
    "            Performs a forward pass on the network.\n",
    "\n",
    "            Parameters:\n",
    "                - state (T.Tensor): The input state.\n",
    "\n",
    "            Returns:\n",
    "                tuple[T.Tensor, T.Tensor]: The value and advantage outputs of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.fc(state)\n",
    "\n",
    "        return self.value(x), self.advantage(x)\n",
    "\n",
    "    def save_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "            Saves the network's checkpoint.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        print('\\tSaving checkpoint...')\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def save_best(self, final_state: tuple) -> None:\n",
    "        \"\"\"\n",
    "            Saves the network's checkpoint with the best score for a given final state.\n",
    "\n",
    "            Parameters:\n",
    "                - final_state (tuple): The final state of the environment.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'\\tSaving {self.name} with best score...')\n",
    "        T.save(\n",
    "            self.state_dict(), \n",
    "            os.path.join(self.chkpt_dir, f'{self.name}_{final_state}')\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "            Loads the network's checkpoint file.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading checkpoint...')\n",
    "        self.load_state_dict(T.load(f'{self.chkpt_file}_best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "        The Agent class represents an agent that interacts with the environment and learns to make decisions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, gamma: float, epsilon: float, learning_rate: float, n_actions: int, \n",
    "        input_dims: list, mem_size: int, batch_size: int, \n",
    "        eps_min: float = 0.01 , eps_decay: float = 5e-7, \n",
    "        replace: int = 1000, \n",
    "        chkpt_dir: str = 'backup'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "            Initializes the Agent object.\n",
    "\n",
    "            Args:\n",
    "                - gamma (float): Discount factor for future rewards.\n",
    "                - epsilon (float): Exploration rate, determines the probability of taking a random action.\n",
    "                - learning_rate (float): Learning rate for the neural network optimizer.\n",
    "                - n_actions (int): Number of possible actions in the environment.\n",
    "                - input_dims (list): Dimensions of the input state.\n",
    "                - mem_size (int): Size of the replay memory buffer.\n",
    "                - batch_size (int): Number of samples to train on in each learning iteration.\n",
    "                - eps_min (float, optional): Minimum value for epsilon. Defaults to 0.01.\n",
    "                - eps_decay (float, optional): Decay rate for epsilon. Defaults to 5e-7.\n",
    "                - replace (int, optional): Number of steps before updating the target network. Defaults to 1000.\n",
    "                - chkpt_dir (str, optional): Directory to save checkpoints. Defaults to 'backup'.\n",
    "        \"\"\"\n",
    "       \n",
    "        self. epsilon = epsilon\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_size = mem_size\n",
    "\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        self.replace_target_cnt = replace\n",
    "        self.learn_step_cnt = 0\n",
    "\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "\n",
    "        self.action_space = [ action for action in range(self.n_actions) ]\n",
    "        self.memory = ReplayBuffer(self.mem_size, self.input_dims)\n",
    "\n",
    "        self.q_eval = DuelingDeepNetwork(\n",
    "            self.lr, self.n_actions, self.input_dims,\n",
    "            'q_eval',\n",
    "            self.chkpt_dir\n",
    "        )\n",
    "\n",
    "        self.q_next = DuelingDeepNetwork(\n",
    "            self.lr, self.n_actions, self.input_dims,\n",
    "            'q_next',\n",
    "            self.chkpt_dir\n",
    "        )\n",
    "\n",
    "    def choose_action(self, observation: list) -> tuple[int, str]:\n",
    "        \"\"\"\n",
    "            Choose an action based on the given observation.\n",
    "\n",
    "            Parameters:\n",
    "                observation (list): The current observation.\n",
    "\n",
    "            Returns:\n",
    "                tuple[int, str]: A tuple containing the chosen action and its type.\n",
    "                The first element is the action (an integer), and the second element is the action type (a string).\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # NN action\n",
    "            state = T.tensor(np.array([observation]), dtype=T.float).to(self.q_eval.device)\n",
    "\n",
    "            _, advantage = self.q_eval.forward(state)\n",
    "            \n",
    "            action = T.argmax(advantage).item()\n",
    "            action_type = 'NN'\n",
    "\n",
    "        else:\n",
    "            # Random action\n",
    "            action = np.random.choice(self.action_space)\n",
    "            action_type = 'Rand'\n",
    "\n",
    "        return action, action_type\n",
    "\n",
    "    def store_transition(self, state: np.array, action: int, reward: float, state_, done: bool) -> None:\n",
    "        \"\"\"\n",
    "            Stores a transition in the replay memory buffer.\n",
    "\n",
    "            Parameters:\n",
    "                - state (np.array): The current state of the environment.\n",
    "                - action (int): The action taken in the current state.\n",
    "                - reward (float): The reward received for taking the action.\n",
    "                - state_ (np.array): The next state of the environment.\n",
    "                - done (bool): Indicates whether the episode is done after taking the action.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def replace_target_network(self) -> None:\n",
    "        \"\"\"\n",
    "            Replaces the target network with the evaluation network.\n",
    "\n",
    "            This method is called periodically to update the target network with the weights of the evaluation network.\n",
    "            The target network is used to estimate the Q-values for the next state during the training process.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.learn_step_cnt % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "            self.learn_step_cnt = 0\n",
    "\n",
    "    def decrement_epsilon(self) -> None:\n",
    "        \"\"\"\n",
    "            Decrements the value of epsilon by eps_decay if epsilon is greater than eps_min.\n",
    "            If epsilon is already less than or equal to eps_min, it is set to eps_min.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def save_models(self) -> None:\n",
    "        \"\"\"\n",
    "            Saves the models' checkpoints.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def save_best(self, final_state: tuple) -> None:\n",
    "        \"\"\"\n",
    "            Saves the models' checkpoints with the best score for a given final state.\n",
    "\n",
    "            Parameters:\n",
    "                - final_state (tuple): The final state of the environment.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_eval.save_best(final_state)\n",
    "        self.q_next.save_best(final_state)\n",
    "\n",
    "    def load_models(self) -> None:\n",
    "        \"\"\"\n",
    "            Loads the models' checkpoints.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def reset_memory(self) -> None:\n",
    "        \"\"\"\n",
    "            Resets the replay memory buffer.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory = ReplayBuffer(self.mem_size, self.input_dims)\n",
    "\n",
    "    def learn(self) -> float:\n",
    "        \"\"\"\n",
    "            Performs the learning process by randomly sampling the memory buffer to retrieve a batch_size sequence of actions.\n",
    "            It then applies the learning equations to update the network weights.\n",
    "\n",
    "            Returns:\n",
    "                float: The loss value after the learning process.\n",
    "        \"\"\"\n",
    "\n",
    "        # Wait until there have been batch size memory episodes \n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return np.nan\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        state, action, reward, next_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states  = T.tensor(state).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        states_ = T.tensor(next_state).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        V_s, A_s = self.q_eval.forward(states)\n",
    "        \n",
    "        V_s_eval, A_s_eval = self.q_eval.forward(states_)\n",
    "\n",
    "        V_s_, A_s_ = self.q_next.forward(states_)\n",
    "\n",
    "        q_pred = T.add(V_s, (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]\n",
    "\n",
    "        q_next = T.add(V_s_, (A_s_ - A_s_.mean(dim=1, keepdim=True)))\n",
    "        q_eval = T.add(V_s_eval, (A_s_eval - A_s_eval.mean(dim=1, keepdim=True)))\n",
    "\n",
    "        max_actions = T.argmax(q_eval, dim=1)\n",
    "\n",
    "        # Value rewards for which the next state is terminal\n",
    "        q_eval[dones] = 0.0\n",
    "\n",
    "        q_target = rewards + self.gamma * q_next[indices, max_actions]\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_cnt += 1\n",
    "\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_heatmap(heatmap: np.array, episode_path: list[tuple]) -> None:\n",
    "    \"\"\"\n",
    "        Increments the heatmap of visited cells based on the given episode path.\n",
    "\n",
    "        Args:\n",
    "            heatmap (np.array): The heatmap to be incremented.\n",
    "            episode_path (list[tuple]): The path taken during the episode.\n",
    "\n",
    "        Returns:\n",
    "            None, the heatmap is modified in place.\n",
    "    \"\"\"\n",
    "\n",
    "    positions_count = Counter(episode_path)\n",
    "\n",
    "    for (row, col), count in positions_count.items():\n",
    "        heatmap[row][col] += count\n",
    "\n",
    "def generate_path_matrix(episode_path: list[tuple]) -> np.array:\n",
    "    \"\"\"\n",
    "        Generates a matrix representing the path taken during an episode.\n",
    "\n",
    "        Args:\n",
    "            episode_path (list[tuple]): The path taken during the episode.\n",
    "\n",
    "        Returns:\n",
    "            np.array: A matrix representing the path taken during the episode.\n",
    "    \"\"\"\n",
    "\n",
    "    matrix = np.zeros((5, 5), dtype=int)\n",
    "   \n",
    "    positions_count = Counter(episode_path)\n",
    "\n",
    "    for (row, col), count in positions_count.items():\n",
    "        matrix[row][col] += count\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def is_cardinal_sequence(episode_path: list[tuple]) -> bool:\n",
    "    \"\"\"\n",
    "        Checks whether the episode path is a cardinal sequence, meaning that all \n",
    "        states in the sequence are in one of the four cardinal positions of the preceding state..\n",
    "\n",
    "        Args:\n",
    "            episode_path (list[tuple]): The path taken during the episode.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the episode path is a cardinal sequence, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(episode_path) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Cardinal steps\n",
    "    cardinal_steps = [(0, 0), (0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "    \n",
    "    for idx in range(1, len(episode_path)):\n",
    "        x_diff = episode_path[idx][0] - episode_path[idx - 1][0]\n",
    "        y_diff = episode_path[idx][1] - episode_path[idx - 1][1]\n",
    "        \n",
    "        # Check if the difference between coordinates matches a cardinal step\n",
    "        if (x_diff, y_diff) not in cardinal_steps:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def not_worse_path(path_len: int, treasure: int, converged_paths: dict) -> bool:\n",
    "    \"\"\"\n",
    "        Checks whether the given path is not worse than the current best path to a given the treasure.\n",
    "\n",
    "        Args:\n",
    "            path_len (int): The length of the path to the treasure.\n",
    "            treasure (int): The treasure to reach.\n",
    "            converged_paths (dict): A dictionary containing the current best paths to the treasures.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the given path is not worse than the current best path to the treasure, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    if treasure in [ solution[1] for solution in converged_paths.keys() ]:\n",
    "\n",
    "        current_len_for_treasure = next(solution for solution in converged_paths.keys() if solution[1] == treasure)[0]\n",
    "\n",
    "        if path_len > current_len_for_treasure: return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning(\n",
    "    scores: list[float], \n",
    "    epsilons: list[float], \n",
    "    losses: list[float], \n",
    "    actions_history: list[dict], \n",
    "    heatmap: np.array,\n",
    "    converged_episodes: dict[tuple, int], \n",
    "    env_img: np.array,\n",
    "    filename: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "        Plots the learning progression and visualizes the environment state.\n",
    "\n",
    "        Args:\n",
    "            scores (list[float]): List of scores for each episode.\n",
    "            epsilons (list[float]): List of epsilon values for each episode.\n",
    "            losses (list[float]): List of mean episode losses for each episode.\n",
    "            actions_history (list[dict]): List of dictionaries containing the count of actions types for each episode.\n",
    "            heatmap (np.array): 2D matrix representing the position visitation heatmap.\n",
    "            converged_episodes (dict[tuple, int]): Dictionary mapping final states to the episode number at which they converged.\n",
    "            env_img (np.array): 2D matrix image representing the final environment state.\n",
    "            filename (str, optional): Name of the file to save the plots. Defaults to None.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    \n",
    "    _, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 9))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    axes[0].plot(scores, color='C0')\n",
    "\n",
    "    if len(converged_episodes):\n",
    "        for episode, path_len in converged_episodes.items():\n",
    "            axes[0].axvline(\n",
    "                episode, alpha=0.5, \n",
    "                ls='--', c=np.random.random(3,), \n",
    "                label=f'Converged to {path_len}'\n",
    "            )\n",
    "\n",
    "        axes[0].legend(loc='lower right')\n",
    "    \n",
    "    axes[0].set(\n",
    "        title='Score progression',\n",
    "        xlabel='Episode',\n",
    "        ylabel='Score'\n",
    "    )\n",
    "\n",
    "    axes[1].plot(epsilons, color='C1')\n",
    "    axes[1].set(\n",
    "        title=r'$\\epsilon$ progression',\n",
    "        xlabel='Episode',\n",
    "        ylabel=r'$\\epsilon$'\n",
    "    )\n",
    "\n",
    "    axes[2].plot(losses, color='C2')\n",
    "    axes[2].set(\n",
    "        title='Loss progression',\n",
    "        xlabel='Episode',\n",
    "        ylabel='Mean Episode Loss'\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(actions_history, columns=['Rand', 'NN',  'Forced']).fillna(0)\n",
    "    \n",
    "    axes[3].plot(df.Rand, c='C3', label='Random')\n",
    "    axes[3].plot(df.Forced, c='C8',label='Forced')\n",
    "    axes[3].plot(df.NN, c='C4',label='Neural\\nNetwork')\n",
    "    \n",
    "    axes[3].legend()\n",
    "    axes[3].set(\n",
    "        title='Type of actions performed',\n",
    "        xlabel='Episode',\n",
    "        ylabel='Quantity',\n",
    "        yscale='log'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename.replace('.png', '_histories.png'))\n",
    "\n",
    "    sns.heatmap(\n",
    "        heatmap, \n",
    "        annot=True, \n",
    "        square=True,\n",
    "        fmt='.2g', \n",
    "        linewidth=.5, \n",
    "        cmap='viridis',\n",
    "        ax=axes[4]\n",
    "    )\n",
    "\n",
    "    axes[4].xaxis.tick_top()\n",
    "    axes[4].set(\n",
    "        title='Position visitation',\n",
    "        xlabel='$x$',\n",
    "        ylabel='$y$'\n",
    "    )\n",
    "\n",
    "    axes[5].imshow(env_img)\n",
    "    axes[5].set_title(\"Episode's Final Env State\")\n",
    "    axes[5].xaxis.set_visible(False)\n",
    "    axes[5].yaxis.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalarized pareto front: [61.0, 93.5, 104.0, 133.5, 176.5, 191.5, 216.5]\n"
     ]
    }
   ],
   "source": [
    "pareto_front = np.array([\n",
    "    [8, 130],\n",
    "    [8, 195],\n",
    "    [8, 216],\n",
    "    [8, 275],\n",
    "    [8, 361],\n",
    "    [8, 391],\n",
    "    [8, 441],\n",
    "])\n",
    "\n",
    "pareto_front_rewards = list(map(\n",
    "    lambda point: np.dot([-0.5, 0.5], point),\n",
    "    pareto_front\n",
    "))\n",
    "\n",
    "print(f'Scalarized pareto front: {pareto_front_rewards}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1, r2, r3, r4 = 80, 145, 166, 175\n",
    "\n",
    "treasure_map = {\n",
    "    50: 'Ø',\n",
    "    r1 + 50: 'R1',\n",
    "    r2 + 50: 'R2',\n",
    "    r3 + 50: 'R3', \n",
    "    r4 + 50: 'R4',\n",
    "\n",
    "    (r1 + r2) + 50: 'R1-R2',\n",
    "    (r1 + r3) + 50: 'R1-R3',\n",
    "    (r1 + r4) + 50: 'R1-R4',\n",
    "    (r2 + r3) + 50: 'R2-R3',\n",
    "    (r2 + r4) + 50: 'R2-R4',\n",
    "    (r3 + r4) + 50: 'R3-R4',\n",
    "\n",
    "    (r1 + r2 + r3) + 50: 'R1-R2-R3',\n",
    "    (r1 + r2 + r4) + 50: 'R1-R2-R4',\n",
    "    (r1 + r3 + r4) + 50: 'R1-R3-R4',\n",
    "    (r2 + r3 + r4) + 50: 'R2-R3-R4',\n",
    "\n",
    "    (r1 + r2 + r3 + r4) + 50: 'R1-R2-R3-R4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_env(\n",
    "    env: gym.Env, agent: Agent, \n",
    "    conversion_threshold: int = 300,\n",
    "    n_trial: int = 1,\n",
    "    write_results: bool = False, \n",
    ") -> None:\n",
    "    \"\"\"\n",
    "        Learn the environment using the Deep Q-Managed algorithm.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment to train the agent in.\n",
    "            agent (Agent): The agent to be trained.\n",
    "            conversion_threshold (int, optional): The threshold for considering a path as converged. Defaults to 300.\n",
    "            n_trial (int, optional): The number of trials. Defaults to 1.\n",
    "            write_results (bool, optional): Whether to write the results. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    n_episodes = 100000\n",
    "\n",
    "    scores, eps_history = [], []\n",
    "    loss_history, episode_losses = [], []\n",
    "    actions_history = []\n",
    "\n",
    "    # Saves the paths taken in the episodes to check for converged states\n",
    "    paths_hashs = Counter()\n",
    "\n",
    "    # Saves the paths of conversion  to an optimal solution { solution_final_reward: path }\n",
    "    converged_paths = {}\n",
    "    removed_converged_paths = {}\n",
    "\n",
    "    # Saves the episode of conversion to an optimal solution: { episode: path_length }\n",
    "    converged_episodes = {}\n",
    "\n",
    "    # Prevents the agent from reaching solutions with the same evaluation\n",
    "    converged_evals = []\n",
    "\n",
    "    # Map space tresure locations\n",
    "    treasure_locs = set([ (0, 2), (2, 2), (4, 2), (4, 0) ])\n",
    "\n",
    "    # Expected solution set\n",
    "    expected_solutions = set([\n",
    "        (8, 130), (8, 195), (8, 216), # One treasure\n",
    "        (8, 275), (8, 361), (8, 391), # Two treasures\n",
    "        (8, 441),                     # Three treasures\n",
    "        (12, 616)                     # Four treasures \n",
    "    ])\n",
    "\n",
    "    # Matrix to save the position visitation\n",
    "    heatmap = np.zeros(env.map.shape, dtype=int)\n",
    "\n",
    "    gold_home = 0\n",
    "    got_lost = 0\n",
    "\n",
    "    forced_paths = 0\n",
    "    tried_paths = 0\n",
    "\n",
    "    boosted_paths = 0\n",
    "    boosted_long_paths = 0\n",
    "\n",
    "    last_epsilon_reset = 0\n",
    "    eps_since_valid_path = 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        # Checking if the agent converged all optimal solutions\n",
    "        if len(expected_solutions & set(converged_paths.keys())) >= 7:\n",
    "            print('Agent converged for desired solutions')\n",
    "            break\n",
    "        \n",
    "        observation, _ = env.reset()  \n",
    "\n",
    "        done = False\n",
    "        score = 0\n",
    "        treasure = 0\n",
    "        \n",
    "        episode_losses = [0]\n",
    "        actions_type = []\n",
    "        transitions = []\n",
    "        \n",
    "        episode_path = []\n",
    "        episode_path_hash = None\n",
    "        \n",
    "        stop = False\n",
    "\n",
    "        while not done:       \n",
    "            action, action_type = agent.choose_action(observation)\n",
    "\n",
    "            action_would_take_home = tuple(env.current_pos + env.dir[action]) == tuple(env.final_pos)\n",
    "            found_equivalent_path  = score + 24.5 in converged_evals\n",
    "\n",
    "            current_solution = next((solution for solution in converged_paths.keys() if solution[1] == treasure + 50), False)\n",
    "            better_path = len(episode_path) + 1 < current_solution[0] if current_solution and action_would_take_home else False\n",
    "\n",
    "            # If an improved solution is identified compared to the one currently learned, the existing solution is \n",
    "            # discarded, allowing the agent to pursue the superior alternative\n",
    "            if better_path:\n",
    "                converged_evals = [ eval for eval in converged_evals if eval != score + 24.5]\n",
    "\n",
    "                removed_converged_paths[current_solution] = converged_paths[current_solution]\n",
    "                del converged_paths[current_solution]\n",
    "\n",
    "                old_conversion_ep = next((key for key, val in converged_episodes.items() if val == score + 24.5), None)\n",
    "                del converged_episodes[old_conversion_ep]\n",
    "\n",
    "                break\n",
    "\n",
    "            # Stops the agent from going to the final state when an equivalent solution has already been found\n",
    "            if action_would_take_home and found_equivalent_path and current_solution and not better_path:\n",
    "                \n",
    "                low_visits = np.argwhere(\n",
    "                    (heatmap <= np.mean(heatmap)) &\n",
    "                    ~generate_path_matrix([(0, 0), *episode_path]).astype(bool)\n",
    "                )\n",
    "                new_pos = low_visits[np.random.randint(0, len(low_visits))]\n",
    "\n",
    "                env.set_state(new_pos)\n",
    "                episode_path.append(tuple(new_pos))\n",
    "\n",
    "                action, _ = agent.choose_action(new_pos)\n",
    "                action_type = 'Forced'\n",
    "                forced_paths += 1\n",
    "       \n",
    "            actions_type.append(action_type)\n",
    "\n",
    "            next_observation, reward, done, _, info = env.step(action)\n",
    "\n",
    "            episode_path.append(tuple(env.current_pos))\n",
    "\n",
    "            score += reward\n",
    "            treasure += info['vector_reward'][1]\n",
    "            \n",
    "            agent.store_transition(observation, action, reward, next_observation, int(done))\n",
    "            transitions.append((observation, action, reward, next_observation, int(done)))\n",
    "            \n",
    "            loss = agent.learn()\n",
    "            \n",
    "            episode_losses.append(loss)\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            if len(episode_path) == 100:\n",
    "                got_lost += 1\n",
    "                eps_since_valid_path += 1\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            if is_cardinal_sequence(episode_path) and not_worse_path(len(episode_path), int(treasure), converged_paths):\n",
    "                \n",
    "                print(f'Path with len {len(episode_path)} with treasure {treasure} added to tracker')\n",
    "\n",
    "                eps_since_valid_path = 0\n",
    "\n",
    "                episode_path_hash = f'{hash(str(episode_path))}{len(episode_path):03d}{treasure:05.1f}'\n",
    "                paths_hashs[episode_path_hash] += 1\n",
    "\n",
    "                good_short_path = len(episode_path) == 8 and treasure_locs & set(episode_path)\n",
    "                good_long_path  = 8 < len(episode_path) <= 14 and len(treasure_locs & set(episode_path)) == 4\n",
    "                if (good_short_path or good_long_path) and len(converged_evals):\n",
    "                    print(f'Boost relevance of path with len {len(episode_path)} and treasure {treasure}: {episode_path}')\n",
    "\n",
    "                    boosted_paths += 10\n",
    "                    \n",
    "                    for observation, action, reward, next_observation, done in [ transition for _ in range(10) for transition in transitions ]:\n",
    "                        agent.store_transition(observation, action, reward + 10 if good_short_path else reward + 100, next_observation, done)\n",
    "                    \n",
    "                    for _ in range(10): agent.learn()\n",
    "\n",
    "                    if 8 < len(episode_path) <= 14: boosted_long_paths += 1\n",
    "                \n",
    "        increment_heatmap(heatmap, episode_path)\n",
    "        tried_paths += 1\n",
    "        \n",
    "        if stop: break\n",
    "        \n",
    "        if episode_path[-1] == (4, 4) and treasure_locs & set(episode_path):\n",
    "            gold_home += 1\n",
    "\n",
    "        scores.append(score)    \n",
    "        eps_history.append(agent.epsilon)\n",
    "        loss_history.append(np.nanmean(episode_losses))\n",
    "        actions_history.append(dict(Counter(actions_type)))\n",
    "\n",
    "        if episode - last_epsilon_reset > 1000 or eps_since_valid_path > 100:\n",
    "            \n",
    "            print(f'\\nIncreasing epsilon\\n')\n",
    "\n",
    "            agent.eps_decay = 1e-3\n",
    "            agent.epsilon = 0.7\n",
    "\n",
    "            last_epsilon_reset = episode\n",
    "\n",
    "            eps_history.append(agent.epsilon) \n",
    "        \n",
    "        else:\n",
    "            print(f'Episodes since last epsilon reset: {episode - last_epsilon_reset}\\tEpisodes since last valid path: {eps_since_valid_path}')\n",
    "\n",
    "        _, hash_count = paths_hashs.most_common(1)[0] if paths_hashs else (0, 0)\n",
    "        if hash_count >= conversion_threshold:\n",
    "            \n",
    "            print('\\n___________________')\n",
    "            print('Converged to solution')\n",
    "            print(f\"{episode_path} {reward} {info['vector_reward']}\")\n",
    "            print('___________________\\n\\n')\n",
    "\n",
    "            # Saving episode of conversion\n",
    "            converged_episodes[episode] = score\n",
    "            last_epsilon_reset = episode\n",
    "\n",
    "            # Saving the paths of conversion\n",
    "            converged_paths[(len(episode_path), int(treasure))] = episode_path\n",
    "\n",
    "            # Blocks the agent from reaching solutions with the same evaluation\n",
    "            converged_evals.append(score)\n",
    "\n",
    "            # Increasing agent randomness\n",
    "            agent.eps_decay = 1e-3\n",
    "            agent.epsilon = 0.7   \n",
    "            eps_history.append(agent.epsilon)        \n",
    "\n",
    "            # Reseting paths taken in the episodes \n",
    "            agent.reset_memory()\n",
    "            paths_hashs = Counter()\n",
    "            gold_home = 0\n",
    "            got_lost = 0\n",
    "            tried_paths = 0\n",
    "            forced_paths = 0\n",
    "            boosted_paths = 0\n",
    "\n",
    "            # Saves the agent's weights\n",
    "            agent.save_best((len(episode_path), int(treasure)))\n",
    "\n",
    "        print(f'\\nEpisode {episode} of {n_episodes}')\n",
    "        \n",
    "        treasure_mapping = treasure_map[treasure] if episode_path[-1] == (4, 4) else treasure_map[treasure + 50]\n",
    "        print(f'\\tScore: {score:.2f}\\tTreasure: {int(treasure)} ({treasure_mapping})\\tAVG Score: {np.mean(scores[-100:]):.2f}\\tMean Loss: {loss_history[-1]:3f}\\tEpsilon: {eps_history[-1]:5f}')\n",
    "\n",
    "        top_repetitions = dict(sorted(\n",
    "            Counter([ \n",
    "                (int(path_hash[-8:-5]), float(path_hash[-5:])) for path_hash, reps in paths_hashs.most_common(5) for _ in range(reps)\n",
    "            ]).items()\n",
    "        ))\n",
    "        print(f'\\n\\tUnique valid paths: {len(paths_hashs)} -> Top 5 repetitions: {top_repetitions}\\n')\n",
    "\n",
    "        gold_home_percent = f'{gold_home     / tried_paths * 100:.1f}' if tried_paths > 0 else '-%'\n",
    "        got_lost_percent  = f'{got_lost      / tried_paths * 100:.1f}' if tried_paths > 0 else '-%'\n",
    "        forced_percent    = f'{forced_paths  / tried_paths * 100:.1f}' if tried_paths > 0 else '-%'\n",
    "        boosted_percent   = f'{boosted_paths / tried_paths * 100:.1f}' if tried_paths > 0 else '-%'\n",
    "        print(f'\\tReturned home with gold: {gold_home} ({gold_home_percent}%)\\tGot lost: {got_lost} ({got_lost_percent}%)\\tPaths with forced actions: {forced_paths} ({forced_percent}%)\\tBoosted Paths: {boosted_paths} ({boosted_percent}%)')\n",
    "        \n",
    "        print(f'\\tLatest episode path length: {len(episode_path)} {episode_path if len(episode_path) < 12 else f\"[{episode_path[0]}, ..., {episode_path[-1]}]\"}')\n",
    "\n",
    "        print('\\n\\tActions taken in episode: NN: {NN}, Rand: {Rand}, Forced: {Forced}'.format_map(Counter(actions_type)))\n",
    "        \n",
    "        print(f'\\n\\tConverged epsiodes: {converged_episodes}')\n",
    "\n",
    "        print(f'\\n\\tConverged solutions: {list(converged_paths.keys())}')\n",
    "\n",
    "        print(f'\\n\\tBoosted long path: {boosted_long_paths}')\n",
    "\n",
    "        print('\\n')\n",
    "        print(generate_path_matrix(episode_path))\n",
    "        print(heatmap)\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if write_results:\n",
    "\n",
    "            with open(f'rg_solutions/solution_rg_{conversion_threshold}_{n_trial}.txt', 'a') as solution_file:\n",
    "                \n",
    "                discovered_front = ' '.join([ \n",
    "                    f'{-1 * path_len} {treasure}' if path else '0 0'\n",
    "                    for (path_len, treasure), path in sorted({\n",
    "                        **converged_paths,\n",
    "                        **{ key: [] for key in expected_solutions.difference(set(converged_paths.keys())) }\n",
    "                    }.items())\n",
    "                ])\n",
    "\n",
    "                solution_file.write(f'{discovered_front}\\n')\n",
    "\n",
    "    print(f'Done with {episode}/{n_episodes}')\n",
    "    print(f'\\n\\tConverged epsiodes: {converged_episodes}')\n",
    "    print(f'\\n\\tConverged paths: {converged_paths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Resource Gathering Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_env = mo_gym.make('modified-resource-gathering-v0', render_mode='rgb_array')\n",
    "mrg_env = mo_gym.LinearReward(mrg_env, weight=np.array([0.5, 0.5]))\n",
    "\n",
    "agent = Agent(\n",
    "    gamma=0.8, \n",
    "    epsilon=1.0, eps_decay=3e-3,\n",
    "    learning_rate=1e-4, \n",
    "    n_actions=4, \n",
    "    input_dims=[2], \n",
    "    mem_size=10000, \n",
    "    batch_size=10, \n",
    "    replace=500,\n",
    "    chkpt_dir = 'backup'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "scores, eps_history, loss_history, actions_history, heatmap, converged_episodes = learn_env(\n",
    "    mrg_env, agent, \n",
    "    100, 1, False\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Elapsed time: {(end_time - start_time)/60:.3f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_env.reset()\n",
    "\n",
    "plot_learning(\n",
    "    scores, \n",
    "    eps_history, loss_history, \n",
    "    actions_history, \n",
    "    heatmap,\n",
    "    converged_episodes, \n",
    "    mrg_env.render()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
