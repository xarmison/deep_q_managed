{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-IHHF3KIGhP"
   },
   "source": [
    "# Deep Sea Treasure Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:39:52.704310Z",
     "start_time": "2024-05-08T03:39:52.689230Z"
    },
    "id": "nCbiajnWIGhR"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Tuple\n",
    "from time import time\n",
    "import base64\n",
    "import os \n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mo_gymnasium as mo_gym\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:39:53.357163Z",
     "start_time": "2024-05-08T03:39:53.317338Z"
    },
    "id": "QUj2BZ0BIGhT"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "        A replay buffer class for storing and sampling transitions for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int, input_shape: list) -> None:\n",
    "        \"\"\"\n",
    "            Initializes the ReplayBuffer class.\n",
    "\n",
    "            Parameters:\n",
    "                - max_size (int): The maximum size of the replay buffer.\n",
    "                - input_shape (list): The shape of the input state.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\" \n",
    "\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "        self.state_memory = np.zeros(\n",
    "            (self.mem_size, *input_shape),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.new_state_memory = np.zeros(\n",
    "            (self.mem_size, *input_shape),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "\n",
    "        # Mask to discount potential features rewards that may come after the current state\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward: float, state_, done: bool) -> None:\n",
    "        \"\"\"\n",
    "            Stores a transition in the replay memory.\n",
    "\n",
    "            Parameters:\n",
    "                - state (np.array): The current state of the environment.\n",
    "                - action (int): The action taken in the current state.\n",
    "                - reward (float): The reward received for taking the action.\n",
    "                - state_ (np.array): The next state of the environment.\n",
    "                - done (bool): Indicates whether the episode is done after taking the action.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        # Index of first free memory\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        # Stores the transition on the memories in the indices in the appropriate arrays\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size: int) -> tuple:\n",
    "        \"\"\"\n",
    "            Randomly samples a batch of transitions from the replay memory buffer.\n",
    "\n",
    "            Args:\n",
    "                batch_size (int): The number of transitions to sample.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple containing the sampled states, actions, rewards, next states, and terminal flags.\n",
    "        \"\"\"\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:39:53.820438Z",
     "start_time": "2024-05-08T03:39:53.768787Z"
    },
    "id": "WtCsw3vYIGhU"
   },
   "outputs": [],
   "source": [
    "class DuelingDeepNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        A class for a dueling deep neural network for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float, n_actions: int, input_dims: list, name: str, chkpt_dir: str) -> None:\n",
    "        \"\"\"\n",
    "            Initializes the DuelingDeepNetwork class.\n",
    "\n",
    "            Parameters:\n",
    "                - learning_rate (float): The learning rate for the optimizer.\n",
    "                - n_actions (int): The number of actions in the environment.\n",
    "                - input_dims (list): The dimensions of the input state.\n",
    "                - name (str): The name of the network.\n",
    "                - chkpt_dir (str): The directory to save the network's checkpoints.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        super(DuelingDeepNetwork, self).__init__()\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        self.chkpt_file = os.path.join(self.chkpt_dir, self.name)\n",
    "        \n",
    "        self.fc1 = nn.Linear(*input_dims, 512)\n",
    "\n",
    "        self.value = nn.Linear(512, 1)\n",
    "        self.advantage = nn.Linear(512, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state: T.Tensor) -> Tuple[T.Tensor, T.Tensor]:\n",
    "        \"\"\"\n",
    "            Performs a forward pass on the network.\n",
    "\n",
    "            Parameters:\n",
    "                - state (T.Tensor): The input state.\n",
    "\n",
    "            Returns:\n",
    "                tuple[T.Tensor, T.Tensor]: The value and advantage outputs of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "\n",
    "        return self.value(x), self.advantage(x)\n",
    "\n",
    "    def save_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "            Saves the network's checkpoint.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        print('\\tSaving checkpoint...')\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def save_best(self, final_state: tuple) -> None:\n",
    "        \"\"\"\n",
    "            Saves the network's checkpoint with the best score for a given final state.\n",
    "\n",
    "            Parameters:\n",
    "                - final_state (tuple): The final state of the environment.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        print(f'\\tSaving {self.name} with best score...')\n",
    "        T.save(\n",
    "            self.state_dict(), \n",
    "            os.path.join(self.chkpt_dir, f'{self.name}_best_{final_state}')\n",
    "        )\n",
    "\n",
    "    def load_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "            Loads the network's checkpoint file.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "        \n",
    "        print('Loading checkpoint...')\n",
    "        self.load_state_dict(T.load(f'{self.chkpt_file}_best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:39:54.424257Z",
     "start_time": "2024-05-08T03:39:54.064269Z"
    },
    "id": "oTxFHHZ7IGhW"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "        The Agent class represents an agent that interacts with the environment and learns to make decisions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, gamma: float, epsilon: float, learning_rate: float, n_actions: int, \n",
    "        input_dims: list, mem_size: int, batch_size: int, \n",
    "        eps_min: float = 0.01 , eps_decay: float = 5e-7, \n",
    "        replace: int = 1000, \n",
    "        chkpt_dir: str = 'dddqn_bk'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "            Initializes the Agent object.\n",
    "\n",
    "            Args:\n",
    "                - gamma (float): Discount factor for future rewards.\n",
    "                - epsilon (float): Exploration rate, determines the probability of taking a random action.\n",
    "                - learning_rate (float): Learning rate for the neural network optimizer.\n",
    "                - n_actions (int): Number of possible actions in the environment.\n",
    "                - input_dims (list): Dimensions of the input state.\n",
    "                - mem_size (int): Size of the replay memory buffer.\n",
    "                - batch_size (int): Number of samples to train on in each learning iteration.\n",
    "                - eps_min (float, optional): Minimum value for epsilon. Defaults to 0.01.\n",
    "                - eps_decay (float, optional): Decay rate for epsilon. Defaults to 5e-7.\n",
    "                - replace (int, optional): Number of steps before updating the target network. Defaults to 1000.\n",
    "                - chkpt_dir (str, optional): Directory to save checkpoints. Defaults to 'backup'.\n",
    "        \"\"\"\n",
    "\n",
    "        self. epsilon = epsilon\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_size = mem_size\n",
    "\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "\n",
    "        self.replace_target_cnt = replace\n",
    "        self.learn_step_cnt = 0\n",
    "\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "\n",
    "        self.action_space = [ action for action in range(self.n_actions) ]\n",
    "        self.memory = ReplayBuffer(self.mem_size, self.input_dims)\n",
    "\n",
    "        self.q_eval = DuelingDeepNetwork(\n",
    "            self.lr, self.n_actions, self.input_dims,\n",
    "            'dst_dddqn_q_eval',\n",
    "            self.chkpt_dir\n",
    "        )\n",
    "\n",
    "        self.q_next = DuelingDeepNetwork(\n",
    "            self.lr, self.n_actions, self.input_dims,\n",
    "            'dst_dddqn_q_next',\n",
    "            self.chkpt_dir\n",
    "        )\n",
    "\n",
    "    def choose_action(self, observation: tuple) -> Tuple[np.array, str]:\n",
    "        \"\"\"\n",
    "            Choose an action based on the given observation.\n",
    "\n",
    "            Parameters:\n",
    "                observation (list): The current observation.\n",
    "\n",
    "            Returns:\n",
    "                tuple[int, str]: A tuple containing the chosen action and its type.\n",
    "                The first element is the action (an integer), and the second element is the action type (a string).\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # NN action\n",
    "            state = T.tensor(np.array([observation]), dtype=T.float).to(self.q_eval.device)\n",
    "\n",
    "            _, advantage = self.q_eval.forward(state)\n",
    "            \n",
    "            action = T.argmax(advantage).item()\n",
    "            action_type = 'NN'\n",
    "\n",
    "        else:\n",
    "            # Random action\n",
    "            action = np.random.choice(self.action_space)\n",
    "            action_type = 'Rand'\n",
    "\n",
    "        return action, action_type\n",
    "\n",
    "    def store_transition(self, state, action, reward: float, state_, done: bool) -> None:\n",
    "        \"\"\"\n",
    "            Stores a transition in the replay memory buffer.\n",
    "\n",
    "            Parameters:\n",
    "                - state (np.array): The current state of the environment.\n",
    "                - action (int): The action taken in the current state.\n",
    "                - reward (float): The reward received for taking the action.\n",
    "                - state_ (np.array): The next state of the environment.\n",
    "                - done (bool): Indicates whether the episode is done after taking the action.\n",
    "\n",
    "            Returns:\n",
    "                - None\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def replace_target_network(self) -> None:\n",
    "        \"\"\"\n",
    "            Replaces the target network with the evaluation network.\n",
    "\n",
    "            This method is called periodically to update the target network with the weights of the evaluation network.\n",
    "            The target network is used to estimate the Q-values for the next state during the training process.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        if self.learn_step_cnt % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "            self.learn_step_cnt = 0\n",
    "\n",
    "    def decrement_epsilon(self) -> None:\n",
    "        \"\"\"\n",
    "            Decrements the value of epsilon by eps_decay if epsilon is greater than eps_min.\n",
    "            If epsilon is already less than or equal to eps_min, it is set to eps_min.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    def save_models(self) -> None:\n",
    "        \"\"\"\n",
    "            Saves the models' checkpoints.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def save_best(self, final_state: tuple) -> None:\n",
    "        \"\"\"\n",
    "            Saves the models' checkpoints with the best score for a given final state.\n",
    "\n",
    "            Parameters:\n",
    "                - final_state (tuple): The final state of the environment.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_eval.save_best(final_state)\n",
    "        self.q_next.save_best(final_state)\n",
    "\n",
    "    def load_models(self) -> None:\n",
    "        \"\"\"\n",
    "            Loads the models' checkpoints.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def learn(self) -> float:\n",
    "        \"\"\"\n",
    "            Performs the learning process by randomly sampling the memory buffer to retrieve a batch_size sequence of actions.\n",
    "            It then applies the learning equations to update the network weights.\n",
    "\n",
    "            Returns:\n",
    "                float: The loss value after the learning process.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Wait until there have been batch size memory episodes \n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return np.nan\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        state, action, reward, next_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states  = T.tensor(state).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        states_ = T.tensor(next_state).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        V_s, A_s = self.q_eval.forward(states)\n",
    "        \n",
    "        V_s_eval, A_s_eval = self.q_eval.forward(states_)\n",
    "\n",
    "        V_s_, A_s_ = self.q_next.forward(states_)\n",
    "\n",
    "        q_pred = T.add(V_s, (A_s - A_s.mean(dim=1, keepdim=True)))[indices, actions]\n",
    "\n",
    "        q_next = T.add(V_s_, (A_s_ - A_s_.mean(dim=1, keepdim=True)))\n",
    "        q_eval = T.add(V_s_eval, (A_s_eval - A_s_eval.mean(dim=1, keepdim=True)))\n",
    "\n",
    "        max_actions = T.argmax(q_eval, dim=1)\n",
    "\n",
    "        # Value rewards for which the next state is terminal\n",
    "        q_eval[dones] = 0.0\n",
    "\n",
    "        q_target = rewards + self.gamma * q_next[indices, max_actions]\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_cnt += 1\n",
    "\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:39:54.529958Z",
     "start_time": "2024-05-08T03:39:54.451234Z"
    },
    "id": "ziZIYA_WIGhY"
   },
   "outputs": [],
   "source": [
    "def increment_heatmap(heatmap: np.array, episode_path: list[tuple]) -> np.array:\n",
    "    \"\"\"\n",
    "        Increments the heatmap of visited cells based on the given episode path.\n",
    "\n",
    "        Args:\n",
    "            heatmap (np.array): The heatmap to be incremented.\n",
    "            episode_path (list[tuple]): The path taken during the episode.\n",
    "\n",
    "        Returns:\n",
    "            None, the heatmap is modified in place.\n",
    "    \"\"\"\n",
    "\n",
    "    positions_count = Counter(episode_path)\n",
    "    del positions_count[(0, 0)]\n",
    "\n",
    "    for (row, col), count in positions_count.items():\n",
    "        heatmap[row][col] += count\n",
    "\n",
    "def plot_learning(\n",
    "    scores: list[float], \n",
    "    epsilons: list[float], \n",
    "    losses: list[float], \n",
    "    actions_history: list[dict], \n",
    "    heatmap: np.array,\n",
    "    converged_episodes: dict[tuple, int], \n",
    "    env_img: np.array,\n",
    "    filename: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "        Plots the learning progression and visualizes the environment state.\n",
    "\n",
    "        Args:\n",
    "            scores (list[float]): List of scores for each episode.\n",
    "            epsilons (list[float]): List of epsilon values for each episode.\n",
    "            losses (list[float]): List of mean episode losses for each episode.\n",
    "            actions_history (list[dict]): List of dictionaries containing the count of random and neural network actions for each episode.\n",
    "            heatmap (np.array): 2D array representing the position visitation heatmap.\n",
    "            converged_episodes (dict[tuple, int]): Dictionary mapping final states to the episode number at which they converged.\n",
    "            env_img (np.array): 2D array representing the final environment state.\n",
    "            filename (str, optional): Output filename for saving the plot. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    \n",
    "    _, axes = plt.subplots(ncols=4, figsize=(18, 4))\n",
    "\n",
    "    axes[0].plot(scores, color='C0')\n",
    "\n",
    "    if len(converged_episodes):\n",
    "        for final_state, episode in converged_episodes.items():\n",
    "            axes[0].axvline(\n",
    "                episode, alpha=0.5, \n",
    "                ls='--', c=np.random.random(3,), \n",
    "                label=f'Converged to {final_state}'\n",
    "            )\n",
    "\n",
    "        # axes[0].legend(loc='center left', bbox_to_anchor=(-0.8, 0.5))\n",
    "        axes[0].legend(loc='lower right')\n",
    "    \n",
    "    axes[0].set(\n",
    "        title='Score progression',\n",
    "        xlabel='Episode',\n",
    "        ylabel='Score'\n",
    "    )\n",
    "\n",
    "    axes[1].plot(epsilons, color='C1')\n",
    "    axes[1].set(\n",
    "        title=r'$\\epsilon$ progression',\n",
    "        xlabel='Episode',\n",
    "        ylabel=r'$\\epsilon$'\n",
    "    )\n",
    "\n",
    "    axes[2].plot(losses, color='C2')\n",
    "    axes[2].set(\n",
    "        title='Loss progression',\n",
    "        xlabel='Episode',\n",
    "        ylabel='Mean Episode Loss'\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(actions_history, columns=['Rand', 'NN']).fillna(0)\n",
    "    \n",
    "    axes[3].plot(df.Rand, c='C3', label='Random')\n",
    "    axes[3].plot(df.NN, c='C4',label='Neural\\nNetwork')\n",
    "    \n",
    "    # axes[3].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axes[3].legend()\n",
    "    axes[3].set(\n",
    "        title='Type of actions performed',\n",
    "        xlabel='Episode',\n",
    "        ylabel='Quantity',\n",
    "        yscale='log'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename.replace('.png', '_histories.png'))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    _, axes = plt.subplots(ncols=2, figsize=(11, 5))\n",
    "\n",
    "    sns.heatmap(\n",
    "        heatmap, \n",
    "        annot=True, \n",
    "        annot_kws={'fontsize': 5},\n",
    "        square=True,\n",
    "        fmt='.2g', \n",
    "        linewidth=.5, \n",
    "        cmap=sns.color_palette(\"blend:#01153E,#FFA266\", as_cmap=True),\n",
    "        ax=axes[0]\n",
    "    )\n",
    "\n",
    "    axes[0].xaxis.tick_top()\n",
    "    axes[0].set(\n",
    "        title='Position visitation',\n",
    "        xlabel='$x$',\n",
    "        ylabel='$y$'\n",
    "    )\n",
    "\n",
    "    axes[1].imshow(env_img)\n",
    "    axes[1].set_title(\"Episode's Final Env State\")\n",
    "    axes[1].xaxis.set_visible(False)\n",
    "    axes[1].yaxis.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename.replace('.png', '_heatmap.png'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:39:54.815420Z",
     "start_time": "2024-05-08T03:39:54.783954Z"
    },
    "id": "KY4vO-tDIGhY"
   },
   "outputs": [],
   "source": [
    "def check_for_unblock(env: gym.Env, action: int, current_path: list, converged_paths: dict[tuple, list], converged_episodes: dict[tuple, int]) -> tuple[dict, dict]:   \n",
    "    \"\"\"\n",
    "        Checks if the current action should unblock a previously converged path and updates the environment accordingly.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment object.\n",
    "            action (int): The action to be taken.\n",
    "            current_path (list): The current path taken by the agent.\n",
    "            converged_paths (dict[tuple, list]): A dictionary containing the converged paths.\n",
    "            converged_episodes (dict[tuple, int]): A dictionary containing the number of episodes for each converged path.\n",
    "\n",
    "        Returns:\n",
    "            tuple[dict, dict]: A tuple containing the updated converged paths and converged episodes dictionaries.\n",
    "    \"\"\"   \n",
    "    \n",
    "    next_state = tuple(env.current_state + env.dir[action])\n",
    "    converged_path = converged_paths.get(next_state, False)\n",
    "\n",
    "    # The +1 represents the additional step that must be done if the current action is carried out.\n",
    "    if converged_path and len(converged_path) > len(current_path) + 1:\n",
    "\n",
    "        env.sea_map[next_state[0], next_state[1]] = env.treasures[next_state]   \n",
    "\n",
    "        new_converged_paths = { key: value for key, value in converged_paths.items() if key != next_state }\n",
    "        new_converged_episodes = { key: value for key, value in converged_episodes.items() if key != next_state }\n",
    "\n",
    "        return new_converged_paths, new_converged_episodes\n",
    "    \n",
    "    return converged_paths, converged_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_env(\n",
    "    env: gym.Env, agent: Agent, \n",
    "    conversion_threshold: int = 300,\n",
    "    n_trial: int = 1,\n",
    "    load_checkpoint: bool = False, write_results: bool = False, \n",
    "    plots_dir: str = 'plots'\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "        Learn the environment using the Deep Q-Managed algorithm.\n",
    "\n",
    "        Args:\n",
    "            env (gym.Env): The environment to learn.\n",
    "            agent (Agent): The agent that learns and interacts with the environment.\n",
    "            conversion_threshold (int, optional): The threshold for considering a state as converged. Defaults to 300.\n",
    "            n_trial (int, optional): The number of trials. Defaults to 1.\n",
    "            load_checkpoint (bool, optional): Whether to load a checkpoint. Defaults to False.\n",
    "            write_results (bool, optional): Whether to write the results to files. Defaults to False.\n",
    "            plots_dir (str, optional): The directory to save the plots. Defaults to 'plots'.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the scores, epsilon history, loss history, actions history, heatmap, and converged episodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_episodes = 100000\n",
    "\n",
    "    if load_checkpoint:\n",
    "        agent.load_models()\n",
    "\n",
    "    scores, eps_history = [], []\n",
    "    loss_history, episode_losses = [], []\n",
    "    actions_history = []\n",
    "\n",
    "    # Saves the paths taken in the episodes to check for converged states\n",
    "    paths_hashs = Counter()\n",
    "\n",
    "    # Saves the paths of conversion to a final state: { (i_final, j_final): path }\n",
    "    converged_paths = { final_state: [] for final_state in env.treasures.keys() }\n",
    "    \n",
    "    # Saves the episode of conversion to a final state: { (i_final, j_final): episode }\n",
    "    converged_episodes = {}\n",
    "\n",
    "    # Matrix to save the position visitation\n",
    "    heatmap = np.zeros(env.sea_map.shape)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        # Checking if the agent converged for treasures states\n",
    "        if np.all(env.sea_map <= 0):\n",
    "            print('Agent converged for all treasure states')\n",
    "            break\n",
    "        \n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        episode_losses = [0]\n",
    "        actions_type = []\n",
    "        \n",
    "        episode_path = []\n",
    "        episode_hash = None\n",
    "\n",
    "        while not done: \n",
    "            action, action_type = agent.choose_action(observation)\n",
    "            actions_type.append(action_type)\n",
    "            \n",
    "            converged_paths, converged_episodes = check_for_unblock(\n",
    "                env, int(action), \n",
    "                episode_path, \n",
    "                converged_paths, converged_episodes\n",
    "            )\n",
    "\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            episode_path.append(tuple(env.current_state))\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            agent.store_transition(observation, action, reward, next_observation, done)\n",
    "            loss = agent.learn()\n",
    "            \n",
    "            episode_losses.append(loss)\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            if len(actions_type) == 1000:\n",
    "                done = True\n",
    "        \n",
    "        else:\n",
    "            increment_heatmap(heatmap, episode_path)\n",
    "\n",
    "            episode_hash = hash(str(episode_path))\n",
    "            paths_hashs[episode_hash] += 1\n",
    "\n",
    "        scores.append(score)    \n",
    "        eps_history.append(agent.epsilon)\n",
    "        loss_history.append(np.nanmean(episode_losses))\n",
    "        actions_history.append(dict(Counter(actions_type)))\n",
    "\n",
    "        _, hash_count = paths_hashs.most_common(1)[0]\n",
    "        if hash_count >= conversion_threshold:\n",
    "            \n",
    "            converged_state = episode_path[-1]\n",
    "\n",
    "            print(f'Converged to state {converged_state}')\n",
    "\n",
    "            # Saving episode of conversion\n",
    "            converged_episodes[converged_state] = episode\n",
    "\n",
    "            # Saving the paths of conversion\n",
    "            converged_paths[converged_state] = episode_path\n",
    "\n",
    "            # Blocking converged state\n",
    "            env.sea_map[converged_state[0], converged_state[1]] = -10\n",
    "\n",
    "            # Increasing agent randomness\n",
    "            agent.epsilon = 0.3   \n",
    "            agent.eps_decay = 1e-3\n",
    "\n",
    "            # Resetting paths taken in the episodes \n",
    "            paths_hashs = Counter()\n",
    "            \n",
    "            if write_results:\n",
    "                agent.save_best(converged_state)  \n",
    "                np.savez(\n",
    "                    f\"{env.name.lower()}_numpys/converged_{converged_state}.npz\",\n",
    "                    scores=scores,\n",
    "                    eps_history=eps_history, \n",
    "                    loss_history=loss_history, \n",
    "                    actions_history=actions_history, \n",
    "                    heatmap=heatmap, \n",
    "                    converged_episodes=converged_episodes,\n",
    "                )\n",
    "    \n",
    "                # Saves the plots for the converged state\n",
    "                env.reset()\n",
    "                plot_learning(\n",
    "                    scores, eps_history, loss_history, \n",
    "                    actions_history, heatmap, converged_episodes, \n",
    "                    env.render(), f'{plots_dir}/plots_converged_{converged_state}.png'\n",
    "                )\n",
    "\n",
    "        print(f'\\nLatest episode length: {len(episode_path)}')\n",
    "        print(f'Paths hashs: {len(paths_hashs)}\\n{paths_hashs}\\n')\n",
    "\n",
    "        print(f'Best path lengths: {env.best_paths_lengths}')\n",
    "        print(f'Converged lengths: { { final_state: len(path) for final_state, path in sorted(converged_paths.items()) } }')\n",
    "        print(f'Converged paths: {converged_paths}')\n",
    "        print(f'Converged episodes: {converged_episodes}')\n",
    "        print(f'Episode {episode} of {n_episodes}\\n\\tScore: {score:.2f} AVG Score: {np.mean(scores[-50:]):.2f} Mean Loss: {loss_history[-1]:3f} Epsilon: {agent.epsilon:5f}')\n",
    "        print('\\tActions taken in episode: NN: {NN}, Rand: {Rand}'.format_map(Counter(actions_type)))\n",
    "        print(f'\\tFinal state: {tuple(observation)}')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # plot_learning(scores, eps_history, loss_history, actions_history, heatmap, converged_episodes, env.render())\n",
    "\n",
    "        if write_results:\n",
    "            env_name = env.name.lower()\n",
    "            with open(f'{env_name}_solutions/solution_{env_name}_{conversion_threshold}_{n_trial}.txt', 'a') as solution_file:\n",
    "                discovered_front = ' '.join([ \n",
    "                    f'{-1 * len(path)} {int(env.treasures[final_state])}' if len(path) else '0 0' \n",
    "                    for final_state, path in sorted(converged_paths.items()) \n",
    "                ])\n",
    "\n",
    "                solution_file.write(f'{discovered_front}\\n')\n",
    "\n",
    "    return scores, eps_history, loss_history, actions_history, heatmap, converged_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Sea Treasure (DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_env = mo_gym.make('deep-sea-treasure-v1', render_mode='rgb_array')\n",
    "dst_env = mo_gym.LinearReward(dst_env, weight=np.array([ 0.5, 0.5 ]))\n",
    "\n",
    "agent = Agent(\n",
    "    gamma=0.99, \n",
    "    epsilon=1.0, eps_decay=3e-3,\n",
    "    learning_rate=1e-4, \n",
    "    n_actions=4, \n",
    "    input_dims=[2], \n",
    "    mem_size=10000, \n",
    "    batch_size=10, \n",
    "    replace=500,\n",
    "    chkpt_dir = 'dst_bk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:40:31.741955Z",
     "start_time": "2024-05-08T03:40:07.246027Z"
    },
    "id": "MXdL_L0CIGhb"
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "scores, eps_history, loss_history, actions_history, heatmap, converged_episodes = learn_env(dst_env, agent, 100, 1, False, False)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Elapsed time: {(end_time - start_time)/60:.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTNBz4jZKixS"
   },
   "outputs": [],
   "source": [
    "dst_env.reset()\n",
    "\n",
    "plot_learning(\n",
    "    scores, \n",
    "    eps_history, loss_history, \n",
    "    actions_history, \n",
    "    heatmap,\n",
    "    converged_episodes, \n",
    "    dst_env.render()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bountiful Sea Treasure (BST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_env = mo_gym.make('bountiful-sea-treasure-v1', render_mode='rgb_array')\n",
    "bst_env = mo_gym.LinearReward(bst_env, weight=np.array([ 0.5, 0.5 ]))\n",
    "\n",
    "agent = Agent(\n",
    "    gamma=0.99, \n",
    "    epsilon=1.0, eps_decay=3e-3,\n",
    "    learning_rate=1e-4, \n",
    "    n_actions=4, \n",
    "    input_dims=[2], \n",
    "    mem_size=10000, \n",
    "    batch_size=10, \n",
    "    replace=500,\n",
    "    chkpt_dir = 'bst_bk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6DIHTRoIGhd"
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "bst_scores, bst_eps_history, bst_loss_history, bst_actions_history, bst_heatmap, bst_converged_episodes = learn_env(bst_env, agent, load_checkpoint=False, plots_dir='bst_plots')\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Elapsed time: {(end_time - start_time)/60:.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y8SudVKLSxa"
   },
   "outputs": [],
   "source": [
    "bst_env.reset()\n",
    "\n",
    "plot_learning(\n",
    "    bst_scores, \n",
    "    bst_eps_history, loss_history, \n",
    "    bst_actions_history, \n",
    "    bst_heatmap,\n",
    "    bst_converged_episodes, \n",
    "    bst_env.render()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Bountiful Sea Treasure (MBST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbst_env = mo_gym.make('modified-bountiful-sea-treasure-v1', render_mode='rgb_array')\n",
    "mbst_env = mo_gym.LinearReward(mbst_env, weight=np.array([ 0.5, 0.5 ]))\n",
    "\n",
    "agent = Agent(\n",
    "    gamma=0.99, \n",
    "    epsilon=1.0, eps_decay=3e-3,\n",
    "    learning_rate=1e-4, \n",
    "    n_actions=4, \n",
    "    input_dims=[2], \n",
    "    mem_size=10000, \n",
    "    batch_size=10, \n",
    "    replace=500,\n",
    "    chkpt_dir = 'mbst_bk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWz1GRSiIGhe"
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "mbst_scores, mbst_eps_history, mbst_loss_history, mbst_actions_history, mbst_heatmap, mbst_converged_episodes = learn_env(mbst_env, agent, False, 'mbst_plots')\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "print(f'Elapsed time: {(end_time - start_time)/60:.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPadd2wqLkxn"
   },
   "outputs": [],
   "source": [
    "mbst_env.reset()\n",
    "\n",
    "plot_learning(\n",
    "    mbst_scores, \n",
    "    mbst_eps_history, loss_history, \n",
    "    mbst_actions_history, \n",
    "    mbst_heatmap,\n",
    "    mbst_converged_episodes, \n",
    "    mbst_env.render()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
